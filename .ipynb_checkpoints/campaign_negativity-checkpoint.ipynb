{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "MidTermsz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nS9vBWoMGHCz"
      },
      "source": [
        "# Importing the required libraries\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAKd009RC2wH",
        "colab_type": "code",
        "outputId": "859a50c1-25dc-48a7-81bc-e39fc492331f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "# After downloading the model, restart the runtime, otherwise it can't be loaded\n",
        "# into memory for some reason."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=c783b353d78ab17683033cc374fd78ee907fdf160c8c9424a591ac0ca66f11d4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s60mdk8y/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b_dxJJRcS521",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FVglrxfvvBT9",
        "colab": {}
      },
      "source": [
        "import re  \n",
        "from sklearn.datasets import load_files  \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import io\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split  \n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jn639iFGGvYn"
      },
      "source": [
        "# Uploading and cleaning the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fk3wNQXrw8fY",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('annotated_tweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WYYOuRnC33su",
        "outputId": "0fd956f0-268f-443a-eb4e-a4e9cccda185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df[\"Text\"][300]\n",
        "# just getting an example of a text field"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Another example of the corruption our current representation is a part of and the false claims that @SenatorCarper is for the environment. Bloomberg energy is a failing \"green\" company that isn\\'t green. Yet is subsidized by millions of your tax dollars. https://t.co/vL1As21gWg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TonRX8iCwqV2",
        "colab": {}
      },
      "source": [
        "X, y = df[\"Text\"], df.iloc[:,0:4]\n",
        "# Splitting the dataset into the text and the lables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CgfLuSYVxYp8",
        "colab": {}
      },
      "source": [
        "# Here we perform all the pre-processing steps and save the output at different stages.\n",
        "\n",
        "documents1 = []\n",
        "documents2 = []\n",
        "documents3 = []\n",
        "documents4 = []\n",
        "\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "for sen in range(0, len(X)):  \n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    documents1.append(document)\n",
        "    \n",
        "    # Lemmatization & stop words\n",
        "    document = document.split()\n",
        "    document2 = [word for word in document if word not in stop_words]\n",
        "    document2 = ' '.join(document2)\n",
        "    documents2.append(document2)\n",
        "    \n",
        "    document3 = [stemmer.stem(word) for word in document]\n",
        "    document3 = ' '.join(document3)\n",
        "    documents3.append(document3)\n",
        "    \n",
        "    document4 = [word for word in document if word not in stop_words]\n",
        "    document4 = [stemmer.stem(word) for word in document4]\n",
        "    document4 = ' '.join(document4)\n",
        "    documents4.append(document4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UWEXTXwr744G",
        "colab": {}
      },
      "source": [
        "documents1 # no lemmas, stop-words kept"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAvXFryKCwVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents2 # no lemmas, stop-words removed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd80WUj-CwVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents3 # lemmas, stop-words kept"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDTBJYOzCwVw",
        "colab_type": "code",
        "outputId": "7bf48a76-ee6f-4c19-9771-ffae62203db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "documents4[300] # lemmas, stop-words remvoed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'exampl corrupt current represent fals claim senatorcarp environ bloomberg energi fail green compani isn green subsid million tax dollar https co vl1as21gwg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eouW4BxhCwV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining the 4 versions\n",
        "all_vers = [documents1, documents2, documents3, documents4] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cHw_HJZFdXb3",
        "outputId": "f413b6ac-bf9e-4422-80bf-7da3303a8ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# transforming the lables into a numpy array\n",
        "# and fixing the one label that had '2' instead of '1'\n",
        "y = y.to_numpy()\n",
        "y = np.where(y==2, 1, y) \n",
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1186, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8eICn_MCwV7",
        "colab_type": "text"
      },
      "source": [
        "# Zero-rule classifier for baseline assessement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIdCg7_ZCwV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple metrics, shows how unbalanced the data is\n",
        "zero_y = [0] * len(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSWetDilCwWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for label in range(0,4):\n",
        "    print(\"LABEL\", label)\n",
        "    print(metrics.classification_report(y[:,label],zero_y))\n",
        "    print(metrics.roc_auc_score(y[:,label], zero_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4nPljarSTYt4"
      },
      "source": [
        "# Word embedding + MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRcQ_jSCwWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function for for word embedding (average per tweet)\n",
        "def nlpfy(x):\n",
        "    nlp_doc = []\n",
        "    for doc in x:\n",
        "        temp_tweet = nlp(doc).vector\n",
        "        nlp_doc.append(temp_tweet)\n",
        "    return nlp_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP7037iLCwWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function that:\n",
        "# 1) splits the data into training and testing\n",
        "# 2) initializes the MLP classifier\n",
        "# 3) trains the model\n",
        "# 4) evaluates the accuracy\n",
        "def fit_model(x, y):\n",
        "    \n",
        "    # Split the data\n",
        "    dX_train2, dX_test2, dy_train2, dy_test2 = train_test_split(x, y, test_size=0.20)\n",
        "    \n",
        "    # Create a space of possible parameters to choose from\n",
        "    parameter_space = {\n",
        "    'hidden_layer_sizes': [(475), (475,237), (100,), (300), (700)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "    \n",
        "    # Fit the data and make predicitons\n",
        "    mlp = MLPClassifier(random_state=0, max_iter=400)\n",
        "    clf2 = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose = 10)\n",
        "    clf2.fit(dX_train2, dy_train2)\n",
        "    predictions2 = clf2.predict(dX_test2)\n",
        "    accu_score = []\n",
        "    for label in range(0,4):\n",
        "        temp_score = metrics.classification_report(dy_test2[:,label],predictions2[:,label])\n",
        "        auc_score = metrics.roc_auc_score(dy_test2[:,label],predictions2[:,label])\n",
        "        accu_score.append((temp_score, auc_score))\n",
        "    return predictions2, accu_score, clf2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx63QWEZCwWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Running the function above\n",
        "we_results = []\n",
        "best_cl = []\n",
        "for vers in all_vers:\n",
        "    docs = nlpfy(vers)\n",
        "    predictions, accu_scores, classer = fit_model(docs, y)\n",
        "    we_results.append((predictions, accu_scores))\n",
        "    best_cl.append(classer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxUIv68IeVX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the output just in case\n",
        "pickle.dump(best_cl, open( \"classifiers.p\", \"wb\" ))\n",
        "pickle.dump(we_results, open( \"we_results.p\", \"wb\" ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZuS-aNDUrqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating the performance\n",
        "for i in range(0,4):\n",
        "  print(\"Docs\", i)\n",
        "  for y in range(0,4):\n",
        "    print(\"label\", y)\n",
        "    print(we_results[i][1][y][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfMJeK-xYBmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The same function as the one above, just for MLPs for inidividual labels.\n",
        "def fit_model_s(x, y):\n",
        "    \n",
        "    # Split the data\n",
        "    dX_train2, dX_test2, dy_train2, dy_test2 = train_test_split(x, y, test_size=0.20)\n",
        "    \n",
        "    # Create a space of possible parameters to choose from\n",
        "    parameter_space = {\n",
        "    'hidden_layer_sizes': [(475), (475,237), (100,), (300), (700)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "    \n",
        "    # Fit the data and make predicitons\n",
        "    mlp = MLPClassifier(random_state=0, max_iter=400)\n",
        "    clf2 = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose = 10)\n",
        "    preds = []\n",
        "    accu_score = []\n",
        "    classers = []\n",
        "\n",
        "    for label in range(0,4):\n",
        "\n",
        "      clf2.fit(dX_train2, dy_train2[:,label])\n",
        "      predictions2 = clf2.predict(dX_test2)\n",
        "      preds.append(predictions2)\n",
        "      classers.append(clf2)\n",
        "\n",
        "      temp_score = metrics.classification_report(dy_test2[:,label],predictions2)\n",
        "      auc_score = metrics.roc_auc_score(dy_test2[:,label],predictions2)\n",
        "      accu_score.append((temp_score, auc_score))\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    preds = preds.transpose()\n",
        "    return preds, accu_score, classers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfHaXDGXa0is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_results_s = []\n",
        "best_cl_s = []\n",
        "for vers in all_vers:\n",
        "    docs = nlpfy(vers)\n",
        "    predictions, accu_scores, classer = fit_model_s(docs, y)\n",
        "    we_results_s.append((predictions, accu_scores))\n",
        "    best_cl_s.append(classer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVcGQPCxSsLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(we_results_s, open( \"we_results_s.p\", \"wb\" ))\n",
        "pickle.dump(best_cls_s, open( \"best_cls_s.p\", \"wb\" ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tkzG1LhoB8TK"
      },
      "source": [
        "# Predictions on complete data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjtPxXz3CkHC",
        "colab": {}
      },
      "source": [
        "# Upload the whole dataset\n",
        "df3 = pd.read_csv('tweets_all_text.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eub_J-wCDRIo",
        "colab": {}
      },
      "source": [
        "# A check to see what it looks like\n",
        "df3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-uK7BN7zgKxj",
        "colab": {}
      },
      "source": [
        "X3 = df3[\"Text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uz746S9NdA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_cls_s = pickle.load(open( \"best_cls_s.p\", \"rb\" ))\n",
        "best_cls = pickle.load(open( \"classifiers.p\", \"rb\" ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZCfey3b-gKxn",
        "colab": {}
      },
      "source": [
        "# Same pre-processing steps as above\n",
        "\n",
        "documents_all_1 = []\n",
        "documents_all_2 = []\n",
        "documents_all_3 = []\n",
        "documents_all_4 = []\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "for sen in range(0, len(X3)):  \n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X3[sen]))\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    documents_all_1.append(document)\n",
        "    \n",
        "    # Lemmatization & stop words\n",
        "    document = document.split()\n",
        "    document2 = [word for word in document if word not in stop_words]\n",
        "    document2 = ' '.join(document2)\n",
        "    documents_all_2.append(document2)\n",
        "    \n",
        "    document3 = [stemmer.stem(word) for word in document]\n",
        "    document3 = ' '.join(document3)\n",
        "    documents_all_3.append(document3)\n",
        "    \n",
        "    document4 = [word for word in document if word not in stop_words]\n",
        "    document4 = [stemmer.stem(word) for word in document4]\n",
        "    document4 = ' '.join(document4)\n",
        "    documents_all_4.append(document4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lIWFJG1xgKx0",
        "colab": {}
      },
      "source": [
        "documents_all = [documents_all_1, documents_all_2, documents_all_3, documents_all_4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGwyGcn9UvV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the predictions\n",
        "predictions0 = best_cl[1].predict(nlpfy(documents_all[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23PPl9lss8V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the predictions\n",
        "pickle.dump(predictions0, open( \"predictions0.p\", \"wb\" ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbk5O88IPF6b",
        "colab": {}
      },
      "source": [
        "# Transfrom that into a pd DataFrame to save it as a csv\n",
        "datasetw = pd.DataFrame({'text': df3[\"Text\"], 'neg_t': predictions0[:,0], 'pol_att': predictions0[:,1], 'per_att': predictions0[:,2], 'inciv': predictions0[:,3]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ynYezOcIPAp3",
        "outputId": "fcb3b4f5-b86f-4447-8894-65c3cbe0c956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Save the csv\n",
        "open('auto_annotated.csv', \"w\").write(datasetw.to_csv())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3449333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}