{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS9vBWoMGHCz"
   },
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "EAKd009RC2wH",
    "outputId": "859a50c1-25dc-48a7-81bc-e39fc492331f"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "# After downloading the model, restart the runtime, otherwise it can't be loaded\n",
    "# into memory for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_dxJJRcS521"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVglrxfvvBT9"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "from sklearn.datasets import load_files  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import io\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jn639iFGGvYn"
   },
   "source": [
    "# Uploading and cleaning the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fk3wNQXrw8fY"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('annotated_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "WYYOuRnC33su",
    "outputId": "0fd956f0-268f-443a-eb4e-a4e9cccda185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Another example of the corruption our current representation is a part of and the false claims that @SenatorCarper is for the environment. Bloomberg energy is a failing \"green\" company that isn\\'t green. Yet is subsidized by millions of your tax dollars. https://t.co/vL1As21gWg'"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text\"][300]\n",
    "# just getting an example of a text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TonRX8iCwqV2"
   },
   "outputs": [],
   "source": [
    "X, y = df[\"Text\"], df.iloc[:,0:4]\n",
    "# Splitting the dataset into the text and the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgfLuSYVxYp8"
   },
   "outputs": [],
   "source": [
    "# Here we perform all the pre-processing steps and save the output at different stages.\n",
    "\n",
    "documents1 = []\n",
    "documents2 = []\n",
    "documents3 = []\n",
    "documents4 = []\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "for sen in range(0, len(X)):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    documents1.append(document)\n",
    "    \n",
    "    # Lemmatization & stop words\n",
    "    document = document.split()\n",
    "    document2 = [word for word in document if word not in stop_words]\n",
    "    document2 = ' '.join(document2)\n",
    "    documents2.append(document2)\n",
    "    \n",
    "    document3 = [stemmer.stem(word) for word in document]\n",
    "    document3 = ' '.join(document3)\n",
    "    documents3.append(document3)\n",
    "    \n",
    "    document4 = [word for word in document if word not in stop_words]\n",
    "    document4 = [stemmer.stem(word) for word in document4]\n",
    "    document4 = ' '.join(document4)\n",
    "    documents4.append(document4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWEXTXwr744G"
   },
   "outputs": [],
   "source": [
    "documents1 # no lemmas, stop-words kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAvXFryKCwVi"
   },
   "outputs": [],
   "source": [
    "documents2 # no lemmas, stop-words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nd80WUj-CwVr"
   },
   "outputs": [],
   "source": [
    "documents3 # lemmas, stop-words kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nDTBJYOzCwVw",
    "outputId": "7bf48a76-ee6f-4c19-9771-ffae62203db6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exampl corrupt current represent fals claim senatorcarp environ bloomberg energi fail green compani isn green subsid million tax dollar https co vl1as21gwg'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents4[300] # lemmas, stop-words remvoed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eouW4BxhCwV0"
   },
   "outputs": [],
   "source": [
    "# combining the 4 versions\n",
    "all_vers = [documents1, documents2, documents3, documents4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cHw_HJZFdXb3",
    "outputId": "f413b6ac-bf9e-4422-80bf-7da3303a8ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1186, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming the lables into a numpy array\n",
    "# and fixing the one label that had '2' instead of '1'\n",
    "y = y.to_numpy()\n",
    "y = np.where(y==2, 1, y) \n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8eICn_MCwV7"
   },
   "source": [
    "# Zero-rule classifier for baseline assessement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIdCg7_ZCwV9"
   },
   "outputs": [],
   "source": [
    "# Simple metrics, shows how unbalanced the data is\n",
    "zero_y = [0] * len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSWetDilCwWB"
   },
   "outputs": [],
   "source": [
    "for label in range(0,4):\n",
    "    print(\"LABEL\", label)\n",
    "    print(metrics.classification_report(y[:,label],zero_y))\n",
    "    print(metrics.roc_auc_score(y[:,label], zero_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nPljarSTYt4"
   },
   "source": [
    "# Word embedding + MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnRcQ_jSCwWG"
   },
   "outputs": [],
   "source": [
    "# A function for for word embedding (average per tweet)\n",
    "def nlpfy(x):\n",
    "    nlp_doc = []\n",
    "    for doc in x:\n",
    "        temp_tweet = nlp(doc).vector\n",
    "        nlp_doc.append(temp_tweet)\n",
    "    return nlp_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uP7037iLCwWN"
   },
   "outputs": [],
   "source": [
    "# A function that:\n",
    "# 1) splits the data into training and testing\n",
    "# 2) initializes the MLP classifier\n",
    "# 3) trains the model\n",
    "# 4) evaluates the accuracy\n",
    "def fit_model(x, y):\n",
    "    \n",
    "    # Split the data\n",
    "    dX_train2, dX_test2, dy_train2, dy_test2 = train_test_split(x, y, test_size=0.20)\n",
    "    \n",
    "    # Create a space of possible parameters to choose from\n",
    "    parameter_space = {\n",
    "    'hidden_layer_sizes': [(475), (475,237), (100,), (300), (700)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "    \n",
    "    # Fit the data and make predicitons\n",
    "    mlp = MLPClassifier(random_state=0, max_iter=400)\n",
    "    clf2 = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose = 10)\n",
    "    clf2.fit(dX_train2, dy_train2)\n",
    "    predictions2 = clf2.predict(dX_test2)\n",
    "    accu_score = []\n",
    "    for label in range(0,4):\n",
    "        temp_score = metrics.classification_report(dy_test2[:,label],predictions2[:,label])\n",
    "        auc_score = metrics.roc_auc_score(dy_test2[:,label],predictions2[:,label])\n",
    "        accu_score.append((temp_score, auc_score))\n",
    "    return predictions2, accu_score, clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx63QWEZCwWQ"
   },
   "outputs": [],
   "source": [
    "# Running the function above\n",
    "we_results = []\n",
    "best_cl = []\n",
    "for vers in all_vers:\n",
    "    docs = nlpfy(vers)\n",
    "    predictions, accu_scores, classer = fit_model(docs, y)\n",
    "    we_results.append((predictions, accu_scores))\n",
    "    best_cl.append(classer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxUIv68IeVX-"
   },
   "outputs": [],
   "source": [
    "# Saving the output just in case\n",
    "pickle.dump(best_cl, open( \"classifiers.p\", \"wb\" ))\n",
    "pickle.dump(we_results, open( \"we_results.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZuS-aNDUrqc"
   },
   "outputs": [],
   "source": [
    "# Evaluating the performance\n",
    "for i in range(0,4):\n",
    "  print(\"Docs\", i)\n",
    "  for y in range(0,4):\n",
    "    print(\"label\", y)\n",
    "    print(we_results[i][1][y][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfMJeK-xYBmc"
   },
   "outputs": [],
   "source": [
    "# The same function as the one above, just for MLPs for inidividual labels.\n",
    "def fit_model_s(x, y):\n",
    "    \n",
    "    # Split the data\n",
    "    dX_train2, dX_test2, dy_train2, dy_test2 = train_test_split(x, y, test_size=0.20)\n",
    "    \n",
    "    # Create a space of possible parameters to choose from\n",
    "    parameter_space = {\n",
    "    'hidden_layer_sizes': [(475), (475,237), (100,), (300), (700)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "    \n",
    "    # Fit the data and make predicitons\n",
    "    mlp = MLPClassifier(random_state=0, max_iter=400)\n",
    "    clf2 = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose = 10)\n",
    "    preds = []\n",
    "    accu_score = []\n",
    "    classers = []\n",
    "\n",
    "    for label in range(0,4):\n",
    "\n",
    "      clf2.fit(dX_train2, dy_train2[:,label])\n",
    "      predictions2 = clf2.predict(dX_test2)\n",
    "      preds.append(predictions2)\n",
    "      classers.append(clf2)\n",
    "\n",
    "      temp_score = metrics.classification_report(dy_test2[:,label],predictions2)\n",
    "      auc_score = metrics.roc_auc_score(dy_test2[:,label],predictions2)\n",
    "      accu_score.append((temp_score, auc_score))\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    preds = preds.transpose()\n",
    "    return preds, accu_score, classers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfHaXDGXa0is"
   },
   "outputs": [],
   "source": [
    "we_results_s = []\n",
    "best_cl_s = []\n",
    "for vers in all_vers:\n",
    "    docs = nlpfy(vers)\n",
    "    predictions, accu_scores, classer = fit_model_s(docs, y)\n",
    "    we_results_s.append((predictions, accu_scores))\n",
    "    best_cl_s.append(classer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVcGQPCxSsLk"
   },
   "outputs": [],
   "source": [
    "pickle.dump(we_results_s, open( \"we_results_s.p\", \"wb\" ))\n",
    "pickle.dump(best_cls_s, open( \"best_cls_s.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tkzG1LhoB8TK"
   },
   "source": [
    "# Predictions on complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjtPxXz3CkHC"
   },
   "outputs": [],
   "source": [
    "# Upload the whole dataset\n",
    "df3 = pd.read_csv('tweets_all_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eub_J-wCDRIo"
   },
   "outputs": [],
   "source": [
    "# A check to see what it looks like\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uK7BN7zgKxj"
   },
   "outputs": [],
   "source": [
    "X3 = df3[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Uz746S9NdA8"
   },
   "outputs": [],
   "source": [
    "best_cls_s = pickle.load(open( \"best_cls_s.p\", \"rb\" ))\n",
    "best_cls = pickle.load(open( \"classifiers.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCfey3b-gKxn"
   },
   "outputs": [],
   "source": [
    "# Same pre-processing steps as above\n",
    "\n",
    "documents_all_1 = []\n",
    "documents_all_2 = []\n",
    "documents_all_3 = []\n",
    "documents_all_4 = []\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "for sen in range(0, len(X3)):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X3[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    documents_all_1.append(document)\n",
    "    \n",
    "    # Lemmatization & stop words\n",
    "    document = document.split()\n",
    "    document2 = [word for word in document if word not in stop_words]\n",
    "    document2 = ' '.join(document2)\n",
    "    documents_all_2.append(document2)\n",
    "    \n",
    "    document3 = [stemmer.stem(word) for word in document]\n",
    "    document3 = ' '.join(document3)\n",
    "    documents_all_3.append(document3)\n",
    "    \n",
    "    document4 = [word for word in document if word not in stop_words]\n",
    "    document4 = [stemmer.stem(word) for word in document4]\n",
    "    document4 = ' '.join(document4)\n",
    "    documents_all_4.append(document4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lIWFJG1xgKx0"
   },
   "outputs": [],
   "source": [
    "documents_all = [documents_all_1, documents_all_2, documents_all_3, documents_all_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGwyGcn9UvV6"
   },
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "predictions0 = best_cl[1].predict(nlpfy(documents_all[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23PPl9lss8V7"
   },
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "pickle.dump(predictions0, open( \"predictions0.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbk5O88IPF6b"
   },
   "outputs": [],
   "source": [
    "# Transfrom that into a pd DataFrame to save it as a csv\n",
    "datasetw = pd.DataFrame({'text': df3[\"Text\"], 'neg_t': predictions0[:,0], 'pol_att': predictions0[:,1], 'per_att': predictions0[:,2], 'inciv': predictions0[:,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ynYezOcIPAp3",
    "outputId": "fcb3b4f5-b86f-4447-8894-65c3cbe0c956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3449333"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the csv\n",
    "open('auto_annotated.csv', \"w\").write(datasetw.to_csv())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MidTermsz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
